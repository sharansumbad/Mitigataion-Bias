{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34f3bcb",
   "metadata": {},
   "source": [
    "## Detecting and mitigating Age and Sex bias on credit decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185feb29",
   "metadata": {},
   "source": [
    "### Importing the Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17d1cb69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21168a",
   "metadata": {},
   "source": [
    "### Load dataset, specifying protected attribute, and split dataset into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d5bef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = GermanDataset(\n",
    "    protected_attribute_names=['age'],                           \n",
    "    privileged_classes=[lambda x: x >= 25],     \n",
    "    features_to_drop=['personal_status', 'sex'] \n",
    "   )\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2671d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988379b",
   "metadata": {},
   "source": [
    "### Compute fairness metric on original training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75a16d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.154574\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d0dff",
   "metadata": {},
   "source": [
    "### Mitigate bias by transforming the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d555bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac1df9",
   "metadata": {},
   "source": [
    "### Compute fairness metric on transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "44d0e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074db48",
   "metadata": {},
   "source": [
    "### The Above cells deteced bias in age and we sucessfully addressed the problem by treating the Bias\n",
    "we have a changed the dataset, we used same measure we used for the original training dataset to check if the baqis was reduced r. The mean difference method in the BinaryLabelDatasetMetric class was  The mitigation step appears to have been quite successful, since the difference in mean outcomes is now 0.0. So, in terms of mean result, we went from a 15 percent advantage for the affluent group to equality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe960f",
   "metadata": {},
   "source": [
    "###  Now we use Sex as an attribute to for detecting and mitigating bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e7ee2",
   "metadata": {},
   "source": [
    "We load datset and set the protected property to sex and drop age as they are not required. The original dataset is then divided into training and testing datasets. Finally, for the privileged (1) in our case we set male  and unprivileged (0) for female values of the sex property, we set  the two variables for identifying and minimizing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4d8b47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_s = GermanDataset(\n",
    "    protected_attribute_names=['sex'],\n",
    "     privileged_classes=[lambda x: x == 'male'],     \n",
    "    features_to_drop=['personal_status', 'age'] \n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8668477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train_s, dataset_orig_test_s = dataset_orig_s.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "85b3e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "93126116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.058012\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train_s, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd4281",
   "metadata": {},
   "source": [
    "The previous step showed that the privileged group was getting 6% more positive outcomes in the training dataset. Since this is not desirable, we are going to try to mitigate this bias in the training dataset, this is called pre-processing mitigation. This means that male is a  postivie class with 6 percent bias next step will be itigating it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39933412",
   "metadata": {},
   "source": [
    "### Mitigate bias by transforming the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e87ea",
   "metadata": {},
   "source": [
    "To mitigate the effects of the gender bias in our original dataset, we can transform the dataset using a pre-processing technique called reweighing. This assigns different weights to the various entities in the population to ensure fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "54d9e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train_s) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008224f",
   "metadata": {},
   "source": [
    "The algorithm we used is Reweighing Algorithm that means this mitigation of biasness will be done before building the model. This algorithm will transform the dataset to have more equity in positive outcomes on the protected attribute for the privileged and unprivileged groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b515cec",
   "metadata": {},
   "source": [
    "### Compute fairness metric on transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9b205aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cf2db",
   "metadata": {},
   "source": [
    "Now that we have a changed dataset, we used same measure we used for the original training dataset to check see how effective it was in reducing bias. The mitigation step appears to have been quite successful, since the difference in mean outcomes is now 0.0. So, in terms of mean result, we went from a 6 percent advantage for the affluent group to equality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb3c55",
   "metadata": {},
   "source": [
    "We've seen how a dataset with historical bias might lead to unjust findings when models are constructed on it. Males would receive greater resources in our scenario since they have traditionally been more likely to acceptable. This is due to the fact that typical machine learning approaches prioritize accuracy above fairness. We've also shown how basic bias mitigation strategies can be used to eliminate bias from datasets, resulting in models with equal accuracy but far higher fairness measures. These strategies for detecting and mitigating bias are critical for any organization looking to automate decision-making on populations with protected characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e21372",
   "metadata": {},
   "source": [
    "### Understanding of Mitigation and Bias\n",
    "\n",
    "we attempt to be sensible when making decisions, assessing the potential benefits and drawbacks of the many options available to make the best decision possible but rather we are more inclined to follow our instincts. Regardless of the technique we choose, there's a good probability you've made some poor choices in your life. While most people assume that making decisions is a reasonable process, research has shown that implicit bias may drive you to particular conclusions without your knowledge. This has ramifications for learning leaders throughout a company.\n",
    "\n",
    "Implicit bias, by its very nature, acts in the subconscious, making it difficult for learning leaders to overcome. Often, you aren't even aware that your objectivity and impartiality are being harmed by bias. \n",
    "\n",
    "Similar situation can be observed when handling a Imbalaced Data set we have major class of target varaible we handle them by ceration ML methods to generate a proper training data set and bais is reduce in our Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a3a81",
   "metadata": {},
   "source": [
    "##### Issues Faced \n",
    "- For chosing a another column 'sex' male was assumed to be privilaged as as this is again a case of bais, here I assume that male is more proviliged class, it was an issue  as why not to choose female to be more postively baised.\n",
    "- comming to technical part the lobrariues imported AI fairness needed Tensorflow for proper functioning\n",
    "- reading the data set to make certain choices was difficult but referred the following [https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)]\n",
    "- while ,oading the data set the datset needed to br the conda envirommet datset folder which was not the case, I downloaded the data set and plces it in the specified path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f5216",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "- https://analyticsindiamag.com/guide-to-ai-fairness-360-an-open-source-toolkit-for-detection-and-mitigation-of-bias-in-ml-models/ \n",
    "- https://www.chieflearningofficer.com/2020/10/22/mitigating-the-effects-of-implicit-bias/\n",
    "- https://ambiata.com/blog/2019-12-13-bias-detection-and-mitigation/\n",
    "- https://nbviewer.org/github/IBM/AIF360/blob/master/examples/tutorial_credit_scoring.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbe2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
